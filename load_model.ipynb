{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845e4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8685c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded model_path from your script\n",
    "model_file_path_str = \"/Users/david/Documents/phd/dev/Papers/Student/allRank-master/out1/full_model.pkl\"\n",
    "model_file_path_obj = Path(model_file_path_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb96aa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to add project root to sys.path: /Users/david/Documents/phd/dev/Papers/Student/allRank-master\n",
      "Successfully added to sys.path: /Users/david/Documents/phd/dev/Papers/Student/allRank-master\n"
     ]
    }
   ],
   "source": [
    "# project_root should be the directory containing the 'allrank' package folder.\n",
    "# If model.pkl is in .../allRank-master/out1/, then .../allRank-master/ is two levels up.\n",
    "project_root_path = model_file_path_obj.parent.parent.resolve() # .resolve() makes it absolute and normalized\n",
    "\n",
    "print(f\"Attempting to add project root to sys.path: {project_root_path}\")\n",
    "\n",
    "if project_root_path.is_dir():\n",
    "    # Check if the 'allrank' package folder exists directly within this project_root_path\n",
    "    allrank_package_path = project_root_path / \"allrank\"\n",
    "    if allrank_package_path.is_dir():\n",
    "        project_root_str = str(project_root_path)\n",
    "        if project_root_str not in sys.path:\n",
    "            sys.path.insert(0, project_root_str)\n",
    "            print(f\"Successfully added to sys.path: {project_root_str}\")\n",
    "        else:\n",
    "            print(f\"Path already in sys.path: {project_root_str}\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected 'allrank' package folder not found at {allrank_package_path}\")\n",
    "        print(\"Ensure your project structure is correct.\")\n",
    "else:\n",
    "    print(f\"Warning: Calculated project root path does not exist or is not a directory: {project_root_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6ebcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, print the current sys.path\n",
    "# print(\"Current sys.path:\")\n",
    "# for p in sys.path:\n",
    "# print(f\"  {p}\")\n",
    "# print(\"-\" * 20)\n",
    "\n",
    "def get_inference_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU)\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon GPU)\")\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cd9580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Loading model from: /Users/david/Documents/phd/dev/Papers/Student/allRank-master/out1/full_model.pkl\n",
      "Model loaded successfully.\n",
      "Model moved to device: mps\n",
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "model_path = model_file_path_str # Use the already defined string\n",
    "device = get_inference_device()\n",
    "loaded_model = None\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file not found at {model_path}\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        # Ensure allrank classes are importable before this line is called\n",
    "        loaded_model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        loaded_model.to(device)\n",
    "        print(f\"Model moved to device: {device}\")\n",
    "\n",
    "        loaded_model.eval()\n",
    "        print(\"Model set to evaluation mode.\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Error loading model: An import error occurred. This often means the model's class definition is not found.\")\n",
    "        print(f\"Ensure the 'allRank' library and its dependencies are correctly installed and importable.\")\n",
    "        print(f\"Specific error: {e}\")\n",
    "        # print(\"Re-check sys.path and project structure if 'allrank' module is not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        # For more detailed debugging, you can uncomment the next two lines:\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb78db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import os # For file existence check, though not strictly needed if path is hardcoded and verified\n",
    "\n",
    "# --- Configuration & Pre-requisites ---\n",
    "svm_file_path = '/Users/david/Documents/phd/JusbrasilData/colecao_especialistas/manual_svm.txt'\n",
    "\n",
    "# !!! CRITICAL: Set this to the number of features your PyTorch model expects.\n",
    "# This should match the dimensionality your model was trained on.\n",
    "N_FEATURES_MODEL_EXPECTS = 11 # Example from your SVM line, adjust if needed for your model\n",
    "\n",
    "# Assume 'loaded_model' is already defined, on the correct 'device', and in eval() mode.\n",
    "# Example (ensure these are correctly set up in your environment before this snippet):\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# # loaded_model = YourModelClass(...) # Or however it was loaded\n",
    "# # loaded_model.to(device)\n",
    "# # loaded_model.eval()\n",
    "\n",
    "# For this script to run, make sure 'loaded_model' and 'device' are available.\n",
    "# If not, you'll need to include their setup. Here's a placeholder check:\n",
    "if 'loaded_model' not in locals() or 'device' not in locals():\n",
    "    print(\"Error: `loaded_model` or `device` is not defined. Please ensure they are set up.\")\n",
    "    # Example setup if missing:\n",
    "    if 'device' not in locals():\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "        else: device = torch.device(\"cpu\")\n",
    "        print(f\"Placeholder: Using device: {device}\")\n",
    "    if 'loaded_model' not in locals():\n",
    "        print(\"Placeholder: `loaded_model` is not loaded. You need to load your model.\")\n",
    "        # As a dummy for the script to proceed without error (replace with your actual model)\n",
    "        class DummyLTRModel(torch.nn.Module):\n",
    "            def __init__(self, n_features): super().__init__(); self.linear = torch.nn.Linear(n_features, 1)\n",
    "            def forward(self, x, mask, indices): return self.linear(x) # Accepts mask & indices\n",
    "        loaded_model = DummyLTRModel(N_FEATURES_MODEL_EXPECTS).to(device).eval()\n",
    "        print(f\"Placeholder: Using a DUMMY model on device {device} in eval mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bc7711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load SVM data from: /Users/david/Documents/phd/JusbrasilData/colecao_especialistas/manual_svm.txt\n",
      "SVM Data loaded: 1311 samples, 11 features.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load SVM Data (Features, Labels, Query IDs)\n",
    "print(f\"Attempting to load SVM data from: {svm_file_path}\")\n",
    "if not os.path.exists(svm_file_path):\n",
    "    print(f\"Error: SVM file not found at {svm_file_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    X_sparse, y_svm_labels, query_ids_svm = load_svmlight_file(\n",
    "        svm_file_path,\n",
    "        n_features=N_FEATURES_MODEL_EXPECTS,\n",
    "        query_id=True,\n",
    "        zero_based=\"auto\" # Handles 0-based or 1-based\n",
    "    )\n",
    "    if X_sparse.shape[0] == 0:\n",
    "        print(\"Error: No samples loaded. Check SVM file path, content, or N_FEATURES_MODEL_EXPECTS.\")\n",
    "        exit()\n",
    "    X_dense_all = X_sparse.toarray()\n",
    "    print(f\"SVM Data loaded: {X_dense_all.shape[0]} samples, {X_dense_all.shape[1]} features.\")\n",
    "    if X_dense_all.shape[1] != N_FEATURES_MODEL_EXPECTS:\n",
    "        print(f\"Warning: Loaded data has {X_dense_all.shape[1]} features, \"\n",
    "              f\"but N_FEATURES_MODEL_EXPECTS is {N_FEATURES_MODEL_EXPECTS}.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading SVM file (ValueError): {e}\")\n",
    "    print(f\"This can happen if n_features ({N_FEATURES_MODEL_EXPECTS}) is smaller than max feature index in file, \"\n",
    "          \"or file is malformed.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading SVM file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81a5be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking X_dense_all for NaN or Inf values...\n",
      "  X_dense_all does not contain any NaN or Inf values. Input data looks clean in that regard.\n",
      "  Min value in X_dense_all: 0.0\n",
      "  Max value in X_dense_all: 88.8873062133789\n"
     ]
    }
   ],
   "source": [
    "# After X_dense_all = X_sparse.toarray()\n",
    "print(f\"Checking X_dense_all for NaN or Inf values...\")\n",
    "nan_check = np.isnan(X_dense_all).any()\n",
    "inf_check = np.isinf(X_dense_all).any()\n",
    "\n",
    "if nan_check or inf_check:\n",
    "    print(f\"!!! Critical Warning: X_dense_all CONTAINS NaN or Inf values! !!!\")\n",
    "    print(f\"  NaNs present: {nan_check} (Total NaNs: {np.sum(np.isnan(X_dense_all))})\")\n",
    "    print(f\"  Infs present: {inf_check} (Total Infs: {np.sum(np.isinf(X_dense_all))})\")\n",
    "    # Optional: Find where they are\n",
    "    # nan_indices = np.where(np.isnan(X_dense_all))\n",
    "    # print(f\"  Indices of NaNs (first few): {list(zip(nan_indices[0][:5], nan_indices[1][:5]))}\")\n",
    "    # inf_indices = np.where(np.isinf(X_dense_all))\n",
    "    # print(f\"  Indices of Infs (first few): {list(zip(inf_indices[0][:5], inf_indices[1][:5]))}\")\n",
    "else:\n",
    "    print(\"  X_dense_all does not contain any NaN or Inf values. Input data looks clean in that regard.\")\n",
    "\n",
    "# Also, check min/max values to understand the range\n",
    "print(f\"  Min value in X_dense_all: {np.min(X_dense_all)}\")\n",
    "print(f\"  Max value in X_dense_all: {np.max(X_dense_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8faaa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG_MODE: Anomaly detection can be enabled if needed (currently commented out).\n",
      "\n",
      "DEBUG_MODE: Checking model parameters for NaNs/Infs...\n",
      "  No NaNs/Infs found in model parameters.\n",
      "\n",
      "Starting inference for 148 unique queries (Target Slate Length: 30)...\n",
      "  DEBUG_MODE: Registering hooks for query 1 (iteration 0)...\n",
      "  Registered 50 hooks.\n",
      "\n",
      "  DEBUG_MODE: Inputs for q_id 1 (iteration 0, actual_docs: 4, docs_in_slate: 4):\n",
      "    X_batched shape: torch.Size([1, 30, 11]), device: mps:0\n",
      "    mask_batched shape: torch.Size([1, 30]), device: mps:0\n",
      "    indices_input shape: torch.Size([1, 30]), device: mps:0\n",
      "  DEBUG_MODE: Removing 50 hooks after processing query 1 (iteration 0).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Assuming pandas, load_svmlight_file, os are imported, and\n",
    "# X_dense_all, query_ids_svm, loaded_model, device, N_FEATURES_MODEL_EXPECTS\n",
    "# are already defined and valid in your environment.\n",
    "\n",
    "# --- Configuration from your JSON & Debugging ---\n",
    "SLATE_LENGTH = 30  # From your model's training config \"data/slate_length\": 30\n",
    "# N_FEATURES_MODEL_EXPECTS should be set correctly (e.g., 11)\n",
    "\n",
    "DEBUG_MODE = True\n",
    "DEBUG_HOOKS_ACTIVE = True\n",
    "QUERIES_TO_DEBUG_WITH_HOOKS = 1 # Process only the first query with hooks for detailed inspection\n",
    "\n",
    "# --- Global variables for hook debugging ---\n",
    "stop_checking_hooks_for_this_pass = False\n",
    "nan_inf_detection_report = None # Stores the first {layer, source, shape, nan, inf} dict\n",
    "\n",
    "# --- PyTorch Hook for NaN/Inf Detection (checks inputs first) ---\n",
    "def nan_check_hook(module, input_args, output_val): # Renamed 'output' to 'output_val'\n",
    "    global stop_checking_hooks_for_this_pass, nan_inf_detection_report\n",
    "\n",
    "    if stop_checking_hooks_for_this_pass: # If NaN already found in this pass, skip\n",
    "        return\n",
    "\n",
    "    current_layer_name = module.__class__.__name__\n",
    "    details_found_this_call = None\n",
    "\n",
    "    # 1. Check INPUTS to this module\n",
    "    if input_args:\n",
    "        for i, inp_tensor in enumerate(input_args):\n",
    "            if isinstance(inp_tensor, torch.Tensor):\n",
    "                is_nan = torch.isnan(inp_tensor).any().item()\n",
    "                is_inf = torch.isinf(inp_tensor).any().item()\n",
    "                if is_nan or is_inf:\n",
    "                    details_found_this_call = {\n",
    "                        'layer': current_layer_name, 'source': f'INPUT_{i}',\n",
    "                        'shape': inp_tensor.shape, 'nan': is_nan, 'inf': is_inf\n",
    "                    }\n",
    "                    break \n",
    "    \n",
    "    # 2. If inputs are clean, check OUTPUT of this module\n",
    "    if not details_found_this_call:\n",
    "        processed_output = False\n",
    "        if isinstance(output_val, torch.Tensor):\n",
    "            is_nan = torch.isnan(output_val).any().item()\n",
    "            is_inf = torch.isinf(output_val).any().item()\n",
    "            if is_nan or is_inf:\n",
    "                details_found_this_call = {\n",
    "                    'layer': current_layer_name, 'source': 'OUTPUT',\n",
    "                    'shape': output_val.shape, 'nan': is_nan, 'inf': is_inf\n",
    "                }\n",
    "            processed_output = True\n",
    "        elif isinstance(output_val, (list, tuple)):\n",
    "            for i, out_tensor in enumerate(output_val):\n",
    "                if isinstance(out_tensor, torch.Tensor):\n",
    "                    is_nan = torch.isnan(out_tensor).any().item()\n",
    "                    is_inf = torch.isinf(out_tensor).any().item()\n",
    "                    if is_nan or is_inf:\n",
    "                        details_found_this_call = {\n",
    "                            'layer': current_layer_name, 'source': f'OUTPUT_{i}',\n",
    "                            'shape': out_tensor.shape, 'nan': is_nan, 'inf': is_inf\n",
    "                        }\n",
    "                        break \n",
    "            processed_output = True\n",
    "        \n",
    "        # If output_val was not a tensor or list/tuple of tensors,\n",
    "        # details_found_this_call would remain None from output check.\n",
    "        # This is fine, as we only care about tensor NaNs.\n",
    "\n",
    "    if details_found_this_call:\n",
    "        nan_inf_detection_report = details_found_this_call \n",
    "        stop_checking_hooks_for_this_pass = True \n",
    "        if DEBUG_MODE:\n",
    "            print(f\"  HOOK >>> NaN/Inf in {details_found_this_call['source']} of/to layer: {details_found_this_call['layer']} \"\n",
    "                  f\"| Shape: {details_found_this_call['shape']} | NaN: {details_found_this_call['nan']}, Inf: {details_found_this_call['inf']}\")\n",
    "\n",
    "# --- Optional: Enable Anomaly Detection (can slow things down) ---\n",
    "if DEBUG_MODE:\n",
    "    # torch.autograd.set_detect_anomaly(True) \n",
    "    print(\"DEBUG_MODE: Anomaly detection can be enabled if needed (currently commented out).\")\n",
    "\n",
    "# --- Check Model Parameters for NaNs/Infs (once before the loop) ---\n",
    "if DEBUG_MODE:\n",
    "    print(\"\\nDEBUG_MODE: Checking model parameters for NaNs/Infs...\")\n",
    "    found_nan_in_params = False\n",
    "    for name, param in loaded_model.named_parameters(): # Ensure loaded_model is defined\n",
    "        if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "            print(f\"  !!! NaN/Inf found in model parameter: {name} !!! Shape: {param.data.shape}\")\n",
    "            found_nan_in_params = True\n",
    "    if not found_nan_in_params:\n",
    "        print(\"  No NaNs/Infs found in model parameters.\")\n",
    "\n",
    "hooks = [] # To store hook handles for removal\n",
    "\n",
    "# 2. Perform Inference using the pre-loaded `loaded_model`\n",
    "all_predictions_list = []\n",
    "unique_q_ids_svm = np.unique(query_ids_svm) # Ensure query_ids_svm is defined\n",
    "print(f\"\\nStarting inference for {len(unique_q_ids_svm)} unique queries (Target Slate Length: {SLATE_LENGTH})...\")\n",
    "\n",
    "loaded_model.eval() # Ensure model is in eval mode\n",
    "with torch.no_grad(): # Context manager for inference\n",
    "    for i, q_id in enumerate(unique_q_ids_svm):\n",
    "        # Reset global flags for each new forward pass (each query in this loop)\n",
    "        stop_checking_hooks_for_this_pass = False\n",
    "        nan_inf_detection_report = None\n",
    "\n",
    "        # Register hooks for the specified number of initial queries\n",
    "        if DEBUG_HOOKS_ACTIVE and i < QUERIES_TO_DEBUG_WITH_HOOKS and not hooks: # Register only once for the debug batch\n",
    "            print(f\"  DEBUG_MODE: Registering hooks for query {q_id} (iteration {i})...\")\n",
    "            for name, module_item in loaded_model.named_modules(): # Corrected variable name to module_item\n",
    "                if name: # Avoid hooking the top-level model itself unnecessarily\n",
    "                    hooks.append(module_item.register_forward_hook(nan_check_hook))\n",
    "            print(f\"  Registered {len(hooks)} hooks.\")\n",
    "        \n",
    "        query_items_indices_in_original = np.where(query_ids_svm == q_id)[0]\n",
    "        X_query_dense_np_orig = X_dense_all[query_items_indices_in_original] # Ensure X_dense_all defined\n",
    "        num_actual_docs = X_query_dense_np_orig.shape[0]\n",
    "\n",
    "        if num_actual_docs == 0:\n",
    "            if DEBUG_MODE: print(f\"  DEBUG_MODE: No documents for q_id {q_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Pad or Truncate features to SLATE_LENGTH ---\n",
    "        X_query_padded_np = np.zeros((SLATE_LENGTH, N_FEATURES_MODEL_EXPECTS), dtype=np.float32)\n",
    "        \n",
    "        # --- Create the mask for valid items (before batching) ---\n",
    "        # Convention: False = valid item, True = padding (to be masked out by attention)\n",
    "        # This convention needs to match how your allRank model's attention layers use the mask.\n",
    "        current_mask_for_slate_np = np.ones(SLATE_LENGTH, dtype=bool) # Initialize all to padding (True)\n",
    "\n",
    "        if num_actual_docs >= SLATE_LENGTH:\n",
    "            X_query_padded_np = X_query_dense_np_orig[:SLATE_LENGTH, :]\n",
    "            current_mask_for_slate_np[:SLATE_LENGTH] = False # All SLATE_LENGTH items are valid\n",
    "            num_docs_in_slate = SLATE_LENGTH\n",
    "        else: # num_actual_docs < SLATE_LENGTH\n",
    "            X_query_padded_np[:num_actual_docs, :] = X_query_dense_np_orig\n",
    "            current_mask_for_slate_np[:num_actual_docs] = False # Mark actual docs as valid\n",
    "            num_docs_in_slate = num_actual_docs # Used for slicing predictions later\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        X_query_tensor_orig = torch.tensor(X_query_padded_np, dtype=torch.float32).to(device) # Ensure device is defined\n",
    "        current_mask_orig = torch.tensor(current_mask_for_slate_np, dtype=torch.bool).to(device) \n",
    "        current_indices_orig = torch.arange(SLATE_LENGTH).to(device) # Indices for the full slate\n",
    "\n",
    "        # Add batch dimension\n",
    "        X_query_tensor_batched = X_query_tensor_orig.unsqueeze(0)   # Shape: [1, SLATE_LENGTH, N_FEATURES]\n",
    "        current_mask_batched = current_mask_orig.unsqueeze(0)    # Shape: [1, SLATE_LENGTH]\n",
    "        current_indices_input = current_indices_orig.unsqueeze(0) # Shape: [1, SLATE_LENGTH]\n",
    "\n",
    "        if DEBUG_MODE and i < QUERIES_TO_DEBUG_WITH_HOOKS:\n",
    "            print(f\"\\n  DEBUG_MODE: Inputs for q_id {q_id} (iteration {i}, actual_docs: {num_actual_docs}, docs_in_slate: {num_docs_in_slate}):\")\n",
    "            print(f\"    X_batched shape: {X_query_tensor_batched.shape}, device: {X_query_tensor_batched.device}\")\n",
    "            print(f\"    mask_batched shape: {current_mask_batched.shape}, device: {current_mask_batched.device}\")\n",
    "            # To see mask values: print(f\"    mask_batched values (first few): {current_mask_batched[0, :num_actual_docs + 2]}\")\n",
    "            print(f\"    indices_input shape: {current_indices_input.shape}, device: {current_indices_input.device}\")\n",
    "\n",
    "        try:\n",
    "            predictions_q_tensor_batched = loaded_model(\n",
    "                X_query_tensor_batched,\n",
    "                mask=current_mask_batched, # This mask (True for padding) is often used to generate attention_mask\n",
    "                indices=current_indices_input\n",
    "            )\n",
    "\n",
    "            if nan_inf_detection_report: # Check if hook found anything during this pass\n",
    "                print(f\"  DEBUG >>> NaN/Inf reported by hooks for q_id {q_id} (see HOOK log above).\")\n",
    "            \n",
    "            # Predictions will be for all SLATE_LENGTH items. Slice to get scores for actual documents.\n",
    "            # The model outputs scores for each of the SLATE_LENGTH positions.\n",
    "            predictions_for_slate_squeezed = predictions_q_tensor_batched.squeeze(0) # Shape: [SLATE_LENGTH, num_output_scores]\n",
    "            \n",
    "            # We only want predictions for the original, non-padded documents\n",
    "            predictions_for_actual_docs = predictions_for_slate_squeezed[:num_actual_docs]\n",
    "            \n",
    "            all_predictions_list.append(predictions_for_actual_docs.cpu().numpy())\n",
    "\n",
    "        except RuntimeError as e_rt:\n",
    "            print(f\"RuntimeError during model inference for q_id {q_id}: {e_rt}\")\n",
    "            print(f\"  Input tensor shapes to model (batched): X={X_query_tensor_batched.shape}, mask={current_mask_batched.shape}, indices={current_indices_input.shape}\")\n",
    "            exit() \n",
    "        except Exception as e_other:\n",
    "            print(f\"An unexpected error for q_id {q_id}: {e_other}\")\n",
    "            # import traceback; traceback.print_exc() # For full trace\n",
    "            exit()\n",
    "        \n",
    "        # Remove hooks if we are done with the debugged queries\n",
    "        if DEBUG_HOOKS_ACTIVE and i == QUERIES_TO_DEBUG_WITH_HOOKS - 1 and hooks:\n",
    "            print(f\"  DEBUG_MODE: Removing {len(hooks)} hooks after processing query {q_id} (iteration {i}).\")\n",
    "            for h in hooks: h.remove()\n",
    "            hooks = [] \n",
    "\n",
    "# Ensure all hooks are removed if loop finishes or breaks early\n",
    "if hooks:\n",
    "    print(f\"  DEBUG_MODE: Cleaning up {len(hooks)} remaining hooks post-loop...\")\n",
    "    for h in hooks: h.remove()\n",
    "    hooks = []\n",
    "\n",
    "if not all_predictions_list:\n",
    "    print(\"No predictions were generated. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# ... (The rest of your script: consolidate predictions, create DataFrame, etc., this part should largely remain the same\n",
    "#       as `all_predictions_list` will now correctly contain arrays of scores for the *actual* number of documents per query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e7b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing and consolidating predictions...\n",
      "\n",
      "DEBUG: Snippet of all_predictions_list (first element, first few scores if available):\n",
      "[1.5303363 1.3989229 1.3987068 1.4303468]\n",
      "Successfully consolidated 1311 scores.\n",
      "  Final `predicted_scores_np` is clean of NaNs.\n",
      "\n",
      "Creating Pandas DataFrame for evaluation...\n",
      "DataFrame created successfully (first 5 rows):\n",
      "   query_id  manual_label  predicted_score\n",
      "0         1           2.0         1.530336\n",
      "1         1           1.0         1.398923\n",
      "2         1           1.0         1.398707\n",
      "3         1           0.0         1.430347\n",
      "4         2           2.0         2.153252\n",
      "\n",
      "Adding predicted ranking to DataFrame...\n",
      "Predicted ranking added (showing head for first few query_ids as sample):\n",
      "\n",
      "Query ID: 1\n",
      "   query_id  manual_label  predicted_score  predicted_ranking\n",
      "0         1           2.0         1.530336                1.0\n",
      "3         1           0.0         1.430347                2.0\n",
      "1         1           1.0         1.398923                3.0\n",
      "2         1           1.0         1.398707                4.0\n",
      "\n",
      "Query ID: 2\n",
      "   query_id  manual_label  predicted_score  predicted_ranking\n",
      "8         2           2.0         2.392813                1.0\n",
      "5         2           2.0         2.153682                4.0\n",
      "4         2           2.0         2.153252                5.0\n",
      "6         2           2.0         2.132409                6.0\n",
      "7         2           2.0         2.121519                7.0\n",
      "\n",
      "Query ID: 3\n",
      "    query_id  manual_label  predicted_score  predicted_ranking\n",
      "18         3           0.0         1.787632                1.0\n",
      "16         3           2.0         1.674398                2.0\n",
      "17         3           1.0         1.637467                4.0\n",
      "15         3           1.0         1.580259                5.0\n",
      "14         3           2.0         1.552532                6.0\n",
      "... and so on for other query_ids.\n",
      "\n",
      "Script section finished at 2025-05-20 14:15:30.660787-04:00\n"
     ]
    }
   ],
   "source": [
    "# (The inference loop from your last message should be directly above this)\n",
    "# It populates `all_predictions_list`\n",
    "\n",
    "# 5. Process and Consolidate Predictions\n",
    "print(\"\\nProcessing and consolidating predictions...\")\n",
    "if not all_predictions_list: # Should have already been checked after the loop, but good for safety\n",
    "    print(\"No predictions were in all_predictions_list. Exiting.\")\n",
    "    exit() # Or return if in a function\n",
    "\n",
    "try:\n",
    "    # Ensure all elements in all_predictions_list are 1D if they represent single scores per doc\n",
    "    # or handle multi-score outputs appropriately before concatenation.\n",
    "    processed_predictions_for_concat = []\n",
    "    for i, pred_array in enumerate(all_predictions_list):\n",
    "        if not isinstance(pred_array, np.ndarray):\n",
    "            print(f\"  Warning: Element {i} in all_predictions_list is not a NumPy array (type: {type(pred_array)}). Skipping this element for concatenation.\")\n",
    "            continue # Skip non-array elements to prevent error during concatenation\n",
    "\n",
    "        if pred_array.ndim > 1 and pred_array.shape[1] == 1: # e.g. shape (num_docs, 1)\n",
    "            processed_predictions_for_concat.append(pred_array.squeeze(axis=1))\n",
    "        elif pred_array.ndim == 1: # e.g. shape (num_docs,)\n",
    "            processed_predictions_for_concat.append(pred_array)\n",
    "        elif pred_array.ndim > 1 and pred_array.shape[1] > 1: # e.g. shape (num_docs, num_classes)\n",
    "            print(f\"  Warning: Prediction array at index {i} has shape {pred_array.shape}. Defaulting to scores from the first column.\")\n",
    "            processed_predictions_for_concat.append(pred_array[:, 0])\n",
    "        elif pred_array.ndim == 0: # Scalar prediction, unlikely for listwise but handle\n",
    "             print(f\"  Warning: Prediction array at index {i} is a scalar ({pred_array}). Wrapping in array.\")\n",
    "             processed_predictions_for_concat.append(np.array([pred_array]))\n",
    "        else: # Other unexpected shapes\n",
    "            print(f\"  Warning: Prediction array at index {i} has an unexpected shape {pred_array.shape}. Attempting to use as is or flatten.\")\n",
    "            processed_predictions_for_concat.append(pred_array.flatten())\n",
    "\n",
    "\n",
    "    if not processed_predictions_for_concat:\n",
    "        print(\"Error: No valid prediction arrays to concatenate after processing.\")\n",
    "        exit()\n",
    "\n",
    "    predicted_scores_np = np.concatenate(processed_predictions_for_concat) # <<<< DEFINED HERE\n",
    "except ValueError as e:\n",
    "    print(f\"Error concatenating predictions: {e}\")\n",
    "    print(\"This might happen if processed prediction arrays for different queries have inconsistent structures that cannot be concatenated.\")\n",
    "    # For debugging, print shapes of arrays in processed_predictions_for_concat:\n",
    "    # for idx, arr in enumerate(processed_predictions_for_concat):\n",
    "    #     print(f\"  Shape of processed prediction array at index {idx}: {arr.shape}\")\n",
    "    exit() # Or return if in a function\n",
    "\n",
    "# Now, predicted_scores_np is defined.\n",
    "\n",
    "# Debug print for all_predictions_list (already in your snippet)\n",
    "if all_predictions_list: # Check if the list itself is not empty\n",
    "    print(\"\\nDEBUG: Snippet of all_predictions_list (first element, first few scores if available):\")\n",
    "    if all_predictions_list[0].size > 0: # Check if the first prediction array is not empty\n",
    "        print(all_predictions_list[0][:min(5, len(all_predictions_list[0]))])\n",
    "    else:\n",
    "        print(\"First element of all_predictions_list is empty or has size 0.\")\n",
    "\n",
    "# Sanity check the total number of scores\n",
    "if len(predicted_scores_np) != X_dense_all.shape[0]: # X_dense_all should be defined from SVM loading\n",
    "    print(f\"!!! Critical Error: Mismatch in the total number of collated prediction scores ({len(predicted_scores_np)}) \"\n",
    "          f\"and the total number of samples in the input data ({X_dense_all.shape[0]}).\")\n",
    "    print(\"This indicates an issue with how predictions were collected or sliced after padding/truncation.\")\n",
    "    exit() # Or return\n",
    "\n",
    "print(f\"Successfully consolidated {len(predicted_scores_np)} scores.\")\n",
    "\n",
    "# Check for NaNs in the final consolidated scores (this was your initial debug print)\n",
    "if np.isnan(predicted_scores_np).any():\n",
    "    nan_count = np.sum(np.isnan(predicted_scores_np))\n",
    "    print(f\"!!! WARNING: Final `predicted_scores_np` contains {nan_count} NaN(s) out of {len(predicted_scores_np)} scores!!!\")\n",
    "else:\n",
    "    print(\"  Final `predicted_scores_np` is clean of NaNs.\")\n",
    "\n",
    "# 6. Create Pandas DataFrame for Evaluation\n",
    "# query_ids_svm and y_svm_labels should be defined from the load_svmlight_file step\n",
    "# and correspond to the original, unpadded/untruncated documents.\n",
    "print(\"\\nCreating Pandas DataFrame for evaluation...\")\n",
    "try:\n",
    "    df_results = pd.DataFrame({\n",
    "        'query_id': query_ids_svm,\n",
    "        'manual_label': y_svm_labels,\n",
    "        'predicted_score': predicted_scores_np\n",
    "    })\n",
    "    print(\"DataFrame created successfully (first 5 rows):\")\n",
    "    print(df_results.head())\n",
    "except ValueError as e:\n",
    "    print(f\"Error creating DataFrame: {e}\")\n",
    "    print(\"This can happen if the lengths of query_ids_svm, y_svm_labels, and predicted_scores_np do not match.\")\n",
    "    print(f\"  len(query_ids_svm): {len(query_ids_svm) if 'query_ids_svm' in locals() else 'Not defined'}\")\n",
    "    print(f\"  len(y_svm_labels): {len(y_svm_labels) if 'y_svm_labels' in locals() else 'Not defined'}\")\n",
    "    print(f\"  len(predicted_scores_np): {len(predicted_scores_np)}\")\n",
    "    exit() # Or return\n",
    "\n",
    "# 7. Add Predicted Ranking\n",
    "print(\"\\nAdding predicted ranking to DataFrame...\")\n",
    "df_results['predicted_ranking'] = df_results.groupby('query_id')['predicted_score'].rank(\n",
    "    ascending=False,  # Higher scores get better rank (e.g., 1st)\n",
    "    method='first'    # Tie-breaking: assign ranks based on order of appearance in group\n",
    ")\n",
    "\n",
    "print(\"Predicted ranking added (showing head for first few query_ids as sample):\")\n",
    "# Display a sample for verification\n",
    "unique_qids_in_results = df_results['query_id'].unique()\n",
    "for q_idx, qid_val in enumerate(unique_qids_in_results):\n",
    "    if q_idx < 3: # Show for the first 3 unique query_ids to keep output concise\n",
    "        print(f\"\\nQuery ID: {qid_val}\")\n",
    "        print(df_results[df_results['query_id'] == qid_val].head().sort_values(by='predicted_ranking'))\n",
    "    else:\n",
    "        break\n",
    "if len(unique_qids_in_results) > 3:\n",
    "    print(\"... and so on for other query_ids.\")\n",
    "\n",
    "print(f\"\\nScript section finished at {pd.Timestamp.now(tz='America/Manaus')}\") # Using your tz from main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af40bfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Snippet of all_predictions_list (first element, first few scores):\n",
      "[1.5303363 1.3989229 1.3987068 1.4303468]\n",
      "No NaNs found in final predicted_scores_np.\n"
     ]
    }
   ],
   "source": [
    "# After the inference loop\n",
    "if all_predictions_list:\n",
    "    print(\"\\nDEBUG: Snippet of all_predictions_list (first element, first few scores):\")\n",
    "    if all_predictions_list[0].size > 0:\n",
    "        print(all_predictions_list[0][:min(5, len(all_predictions_list[0]))])\n",
    "    else:\n",
    "        print(\"First element of all_predictions_list is empty.\")\n",
    "# ... then proceed to concatenate ...\n",
    "# After predicted_scores_np = np.concatenate(all_predictions_list)\n",
    "if np.isnan(predicted_scores_np).any():\n",
    "    print(f\"!!! WARNING: NaNs found in final predicted_scores_np! Count: {np.sum(np.isnan(predicted_scores_np))} !!!\")\n",
    "    # Find which queries might have NaNs if needed\n",
    "    # for k in range(len(all_predictions_list)):\n",
    "    #     if np.isnan(all_predictions_list[k]).any():\n",
    "    #         print(f\"NaNs found in predictions for query index {k} (original q_id might differ due to unique_q_ids_svm sorting)\")\n",
    "else:\n",
    "    print(\"No NaNs found in final predicted_scores_np.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9de7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process and Consolidate Predictions\n",
    "try:\n",
    "    predicted_scores_np = np.concatenate(all_predictions_list)\n",
    "except ValueError as e:\n",
    "    print(f\"Error concatenating predictions: {e}. Check shapes of model outputs from different queries.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ee5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame created (first 5 rows):\n",
      "   query_id  manual_label  predicted_score\n",
      "0         1           2.0         1.530336\n",
      "1         1           1.0         1.398923\n",
      "2         1           1.0         1.398707\n",
      "3         1           0.0         1.430347\n",
      "4         2           2.0         2.153252\n",
      "\n",
      "DataFrame with predicted ranking (showing head for first few query_ids):\n",
      "\n",
      "Query ID: 1\n",
      "   query_id  manual_label  predicted_score  predicted_ranking\n",
      "0         1           2.0         1.530336                1.0\n",
      "3         1           0.0         1.430347                2.0\n",
      "1         1           1.0         1.398923                3.0\n",
      "2         1           1.0         1.398707                4.0\n",
      "\n",
      "Query ID: 2\n",
      "   query_id  manual_label  predicted_score  predicted_ranking\n",
      "8         2           2.0         2.392813                1.0\n",
      "5         2           2.0         2.153682                4.0\n",
      "4         2           2.0         2.153252                5.0\n",
      "6         2           2.0         2.132409                6.0\n",
      "7         2           2.0         2.121519                7.0\n",
      "\n",
      "Query ID: 3\n",
      "    query_id  manual_label  predicted_score  predicted_ranking\n",
      "18         3           0.0         1.787632                1.0\n",
      "16         3           2.0         1.674398                2.0\n",
      "17         3           1.0         1.637467                4.0\n",
      "15         3           1.0         1.580259                5.0\n",
      "14         3           2.0         1.552532                6.0\n",
      "... and so on for other query_ids.\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Pandas DataFrame and Add Rankings\n",
    "df_results = pd.DataFrame({\n",
    "    'query_id': query_ids_svm,         # Original query_ids from SVM file\n",
    "    'manual_label': y_svm_labels,      # Original labels from SVM file\n",
    "    'predicted_score': predicted_scores_np # Scores from your model\n",
    "})\n",
    "\n",
    "print(\"\\nDataFrame created (first 5 rows):\")\n",
    "print(df_results.head())\n",
    "\n",
    "# Add predicted_ranking (similar to your df_test logic)\n",
    "df_results['predicted_ranking'] = df_results.groupby('query_id')['predicted_score'].rank(\n",
    "    ascending=False, # Higher scores get better rank (1st, 2nd, etc.)\n",
    "    method='first'   # Tie-breaking: assign ranks in order of appearance within group\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame with predicted ranking (showing head for first few query_ids):\")\n",
    "# Display a sample for verification\n",
    "unique_qids_in_results = df_results['query_id'].unique()\n",
    "for q_idx, qid_val in enumerate(unique_qids_in_results):\n",
    "    if q_idx < 3: # Show for the first 3 unique query_ids\n",
    "        print(f\"\\nQuery ID: {qid_val}\")\n",
    "        print(df_results[df_results['query_id'] == qid_val].head().sort_values(by='predicted_ranking'))\n",
    "    else:\n",
    "        break\n",
    "if len(unique_qids_in_results) > 3:\n",
    "    print(\"... and so on for other query_ids.\")\n",
    "\n",
    "# Now 'df_results' contains the data in the desired format.\n",
    "# You can save it or use it for further evaluation.\n",
    "# Example: df_results.to_csv(\"evaluation_results.csv\", index=False)\n",
    "# print(\"\\nResults saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure predicted_scores_np is 1D (or becomes 1D after squeeze/selection)\n",
    "if predicted_scores_np.ndim > 1:\n",
    "    if predicted_scores_np.shape[1] == 1: # Shape (N, 1)\n",
    "        predicted_scores_np = predicted_scores_np.squeeze(axis=1)\n",
    "    else: # Shape (N, C) where C > 1, e.g., multi-target or class scores\n",
    "        print(f\"Warning: Predictions have {predicted_scores_np.shape[1]} columns. Using scores from the first column for ranking.\")\n",
    "        predicted_scores_np = predicted_scores_np[:, 0]\n",
    "\n",
    "if len(predicted_scores_np) != X_dense_all.shape[0]:\n",
    "    print(f\"Critical Error: Mismatch in number of collated predictions ({len(predicted_scores_np)}) \"\n",
    "          f\"and total samples ({X_dense_all.shape[0]}).\")\n",
    "    exit()\n",
    "print(f\"\\nInference complete. Generated {len(predicted_scores_np)} total scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e549175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913cf45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@1: 0.6757\n",
      "Average NDCG@3: 0.7093\n",
      "Average NDCG@5: 0.7543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def ndcg_at_k(true_relevance, predicted_rank, k=5):\n",
    "\n",
    "    # Ensure the series are of the same length\n",
    "    assert len(true_relevance) == len(predicted_rank), \"Series must have the same length\"\n",
    "    \n",
    "    # Skip queries with less than k documents\n",
    "    if len(true_relevance) < k:\n",
    "        return np.nan  # Return NaN for queries with fewer than k documents\n",
    "    \n",
    "    # Sort predicted_rank based on the predicted ranking order\n",
    "    sorted_true_relevance = true_relevance.iloc[predicted_rank.argsort()]\n",
    "    \n",
    "    # Limit to the top k positions for DCG calculation\n",
    "    sorted_true_relevance_at_k = sorted_true_relevance.head(k)\n",
    "    \n",
    "    # Calculate DCG@k (Discounted Cumulative Gain) for the top k ranking positions\n",
    "    dcg_k = np.sum((2 ** sorted_true_relevance_at_k.values - 1) / np.log2(np.arange(2, len(sorted_true_relevance_at_k) + 2)))\n",
    "    \n",
    "    # Calculate IDCG@k (Ideal Discounted Cumulative Gain) for the ideal ranking (sorted by true relevance)\n",
    "    ideal_rank_at_k = true_relevance.sort_values(ascending=False).head(k)\n",
    "    idcg_k = np.sum((2 ** ideal_rank_at_k.values - 1) / np.log2(np.arange(2, len(ideal_rank_at_k) + 2)))\n",
    "    \n",
    "    # Calculate NDCG@k\n",
    "    ndcg_k = dcg_k / idcg_k if idcg_k != 0 else 0  # Avoid division by zero\n",
    "    \n",
    "    return ndcg_k\n",
    "\n",
    "# Example of processing the entire dataset\n",
    "\n",
    "\n",
    "ks = [1,3,5]\n",
    "\n",
    "\n",
    "for k in ks:\n",
    "    ndcg_total = 0\n",
    "    valid_queries = 0  # To count queries with at least k documents \n",
    "    for query_id, subdf in df_test.groupby('query_id'):\n",
    "        true_relevance = subdf['manual_label']  # True relevance scores from experts\n",
    "        predicted_rank = subdf['predicted_ranking']  # Predicted ranking positions\n",
    "        \n",
    "        # Calculate NDCG@K for the current query (adjust k as needed)\n",
    "        ndcg_score = ndcg_at_k(true_relevance, predicted_rank, k)  # Use k=5 as an example\n",
    "        \n",
    "        # Only include queries that have at least k documents\n",
    "        if not np.isnan(ndcg_score):  \n",
    "            ndcg_total += ndcg_score\n",
    "            valid_queries += 1\n",
    "\n",
    "    # Average NDCG over all queries with at least k documents\n",
    "    average_ndcg = ndcg_total / valid_queries if valid_queries > 0 else 0\n",
    "    print(f\"Average NDCG@{k}: {average_ndcg:.4f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2693c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
